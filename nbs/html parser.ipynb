{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from scrapy.selector import Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = []\n",
    "# with open('../data/missed_urls.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         lines.append(line.rstrip().split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse title and price from missed urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id_no in lines:\n",
    "#     with open('html/'+id_no[0]+'.html', 'r') as f:\n",
    "#         webpage = f.read()\n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"overview-scroll\"]/div/div/div[2]').extract()\n",
    "#     print(s[0])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract prices with xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# prices = {}\n",
    "# count = 0\n",
    "# for id_no in lines:\n",
    "#     count += 1\n",
    "#     if count % 100 == 0:\n",
    "#         print('Extracting', id_no)\n",
    "\n",
    "#     with open('html/'+id_no[0]+'.html', 'r') as f:\n",
    "#         webpage = f.read()\n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"overview-scroll\"]/div/div/div[2]').extract()\n",
    "#     try:\n",
    "#         price = int(s[0].split('\"name\":\"')[1].split(',')[5].split('\"price\":')[1])\n",
    "#         prices[id_no[0]] = price\n",
    "#     except IndexError:\n",
    "#         prices[id_no[0]] = None\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_df = pd.Series(prices)\n",
    "# display(price_df.head(6))\n",
    "# display(price_df.isnull().sum())\n",
    "# display(price_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_df.to_csv('prices.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_df = pd.read_csv('../data/prices.txt', \n",
    "#                         index_col=0, \n",
    "#                        header=-1)\n",
    "# price_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add name to properly update\n",
    "# price_df.columns = ['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join prices with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('dataset.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">insert prices index and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numeric values from string\n",
    "\n",
    "# df['price'] = df['stats'].str.split('\\n', \n",
    "#                       expand=True)[2].str.replace(r'[\\$,]', \n",
    "#                       '', regex=True)\n",
    "\n",
    "# df['price'] = pd.to_numeric(df['price'], errors='coerce',\n",
    "#              downcast='float')\n",
    "\n",
    "# print(df.loc[5,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximately 6597 listings had null price when queried with xpath v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### fill nulls with prices obtained from html xpath v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['price'] = df.price.fillna(price_df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.price.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract area with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# areas = {}\n",
    "# for j, id_no in enumerate(lines):\n",
    "#     price_per_sf = None\n",
    "#     html_text = None\n",
    "    \n",
    "#     with open('html/'+id_no[0]+'.html', 'r') as f:\n",
    "#         webpage = f.read()\n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"overview-scroll\"]/div/div/div[2]').extract()\n",
    "#     try:\n",
    "#         html_text = s[0]\n",
    "#         area = html_text.split('$')[1].split('<span class=\"statsValue\">')[1].split('</', 1)[0]\n",
    "#         price_per_sf = area\n",
    "#         try:\n",
    "#             numeric = pd.to_numeric(price_per_sf)\n",
    "#         except IndexError:\n",
    "#             print('bad index', id_no)\n",
    "#             price_per_sf = part1\n",
    "#             pass\n",
    "#     except:\n",
    "#         print(\"error\", id_no)\n",
    "#         pass\n",
    "#     areas[id_no[0]] = price_per_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check back to recover other structure\n",
    "# area_df = pd.Series(areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfa = pd.to_numeric(area_df.str.replace(r'[\\$,]', '', \n",
    "#                                   regex=True), \n",
    "#               errors='coerce',\n",
    "#              downcast='float')\n",
    "# dfa.head()\n",
    "# #set name equal for df merge\n",
    "# dfa.name = 'area'\n",
    "# dfa.to_csv('areas.txt')\n",
    "# area_df = pd.read_csv('areas.txt', \n",
    "#                         index_col=0, \n",
    "#                        header=-1)\n",
    "# area_df.head()\n",
    "# #Add name to properly update\n",
    "# area_df.columns = ['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join areas with dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> insert area to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['area'] = df['stats'].str.split('\\n', \n",
    "#                            expand=True)[7].str.split(' Sq. Ft.', \n",
    "#                                                      expand=True)[0].str.split('Baths', \n",
    "#                                                                                expand=True)[1].str.replace(',', '')\n",
    "\n",
    "# # show example nan\n",
    "# df.loc[5,:]\n",
    "# area_df.isnull().sum(), area_df.shape[0]\n",
    "# # show example fill\n",
    "# df['area'].fillna(area_df['area'])[5]\n",
    "# # fill null area\n",
    "# df['area'] = df['area'].fillna(area_df['area'])\n",
    "# df.area[5]\n",
    "# df['area'] = pd.to_numeric(df.area, errors='coerce')\n",
    "# df.area.isnull().sum()\n",
    "# bad_areas = pd.Series(areas)\n",
    "# bad_areas[bad_areas.str.contains(r'View')==True].index\n",
    "# bad_area_urls = bad_areas[bad_areas.str.contains(r'View')==True].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get remainder areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# areas2 = {}\n",
    "# for j, id_no in enumerate(bad_area_urls.tolist()):\n",
    "#     price_per_sf = None\n",
    "#     html_text = None\n",
    "# #     if j % 5 == 0:\n",
    "# #         print('Extracting', id_no)\n",
    "#     with open('html/'+id_no+'.html', 'r') as f:\n",
    "#         webpage = f.read()\n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"basicInfo\"]/div[2]/div[1]/div[5]/div').extract()\n",
    "#     price_per_sf = s[0].split('</div>')[0].split('>')[1]\n",
    "#     areas2[id_no] = price_per_sf\n",
    "    \n",
    "# areas2 = pd.to_numeric(pd.Series(areas2).str.replace(',', ''),\n",
    "#               errors='coerce')\n",
    "# areas2.to_csv('areas2.txt')\n",
    "\n",
    "# dfa2 = pd.read_csv('areas2.txt', \n",
    "#                         index_col=0, \n",
    "#                        header=-1)\n",
    "# dfa2.tail()\n",
    "# dfa2.columns = ['area']\n",
    "# df.area[46]\n",
    "# df['area'].fillna(dfa2['area'])[46]\n",
    "# df['area'] = df['area'].fillna(dfa2['area'])\n",
    "# df.area.isnull().sum()\n",
    "# # df.to_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract interior and exterior features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# features = {}\n",
    "# for j, id_no in enumerate(np.arange(18790)):\n",
    "#     id_no = str(id_no)\n",
    "#     if j%100==0:\n",
    "#         print(id_no)\n",
    "#     with open('html/'+id_no+'.html', 'r') as f:\n",
    "#         webpage = f.read()       \n",
    "#     details = Selector(text=webpage).xpath('//*[@id=\"property-details-scroll\"]/div').extract()\n",
    "#     features[id_no] = details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_series = pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_series.to_csv('features.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Cleaning and extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in df[df.price>2000000]['listing'].values:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['listing'][12820])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract bathrooms count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11104 []\n",
      "18593 []\n",
      "18708 []\n"
     ]
    }
   ],
   "source": [
    "# bathrooms = {}\n",
    "# for j, id_no in enumerate(lines):\n",
    "# #     if j%100==0:\n",
    "# #         print(id_no)\n",
    "#     with open('../data/html/'+id_no[0]+'.html', 'r') as f:\n",
    "#         webpage = f.read()       \n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"overview-scroll\"]/div/div').extract()\n",
    "#     try:\n",
    "#         bathrooms[id_no[0]] = s[0].partition('<div class=\"info-block\" data-rf-test-id=\"abp-baths\"><div class=\"statsValue\">')[2][0]\n",
    "#     except:\n",
    "#         print(id_no[0], id_no[1])\n",
    "#         bathrooms[id_no[0]] = None\n",
    "#         pass\n",
    "\n",
    "# bathrooms = pd.Series(bathrooms, name='bathrooms')\n",
    "\n",
    "# bathrooms = pd.to_numeric(bathrooms, errors='coerce')\n",
    "\n",
    "# bathrooms.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract bedrooms count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedrooms = {}\n",
    "# for j, id_no in enumerate(lines):\n",
    "#     with open('../data/html/'+id_no[0]+'.html', 'r') as f:\n",
    "#         webpage = f.read()       \n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"overview-scroll\"]/div/div').extract()\n",
    "#     try:\n",
    "#         bedrooms[id_no[0]] = s[0].partition('<div class=\"info-block\" data-rf-test-id=\"abp-beds\"><div class=\"statsValue\">')[2][0]\n",
    "#     except:\n",
    "#         print(id_no[0], id_no[1])\n",
    "#         bedrooms[id_no[0]] = None\n",
    "#         pass \n",
    "\n",
    "# bedrooms = pd.Series(bedrooms, name='bedrooms')\n",
    "\n",
    "# bedrooms = pd.to_numeric(bedrooms, errors='coerce')\n",
    "\n",
    "# bedrooms.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate = {}\n",
    "# for j, id_no in enumerate(lines):\n",
    "#     with open('../data/html/'+id_no[0]+'.html', 'r') as f:\n",
    "#         webpage = f.read()       \n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"overview-scroll\"]/div/div').extract()\n",
    "#     try:\n",
    "#         estimate[id_no[0]] = s[0].partition('<span data-rf-test-id=\"avmLdpPrice\" class=\"\"><span class=\"label\"><span class=\"clickable\"><span class=\"avmLabel\">Redfin Estimate:</span> </span></span><span class=\"value\">')[2].split('</span>')[0]\n",
    "#     except:\n",
    "#         print(id_no[0], id_no[1])\n",
    "#         estimate[id_no[0]] = None\n",
    "#         pass \n",
    "\n",
    "# estimate = pd.Series(estimate, name='estimate')\n",
    "# estimate = pd.to_numeric(estimate.str.replace(r'[\\$,]','', regex=True), errors='coerce')\n",
    "\n",
    "# estimate.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status = {}\n",
    "# for j, id_no in enumerate(lines):\n",
    "#     with open('../data/html/'+id_no[0]+'.html', 'r') as f:\n",
    "#         webpage = f.read()       \n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"overview-scroll\"]/div/div').extract()\n",
    "\n",
    "#     try:\n",
    "#         status[id_no[0]] = s[0].partition('<span class=\"status-container\" data-rf-test-id=\"abp-status\"><span><span class=\"label\">Status: </span><span class=\"value\"><div class=\"DefinitionFlyout definition-flyout-container react inline-block\"><span class=\"DefinitionFlyoutLink inline-block underline clickable\">')[2].split('</span>',1)[0]\n",
    "#     except:\n",
    "#         print(id_no[0], id_no[1])\n",
    "#         status[id_no[0]] = None\n",
    "#         pass \n",
    "\n",
    "# status = pd.Series(status, name='status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# info = {}\n",
    "# bad_info = {}\n",
    "# for j, id_no in enumerate(lines):\n",
    "# #     if j%100==0:\n",
    "# #         print(id_no)\n",
    "#     with open('../data/html/'+id_no[0]+'.html', 'r') as f:\n",
    "#         webpage = f.read()       \n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"content\"]/div[10]').extract()\n",
    "#     try:\n",
    "#         info[id_no[0]] = s[0].split('<span>', 1)[1].split('</span>', 1)[0]\n",
    "#     except:\n",
    "#         print(id_no[0], id_no[1])\n",
    "#         bad_info[id_no[0]] = id_no[1]\n",
    "#         info[id_no[0]] = None\n",
    "#         pass \n",
    "\n",
    "# Create bad info file\n",
    "\n",
    "# bad_info = pd.Series(bad_info)\n",
    "\n",
    "# bad_info.to_csv('../data/bad_info.csv')\n",
    "\n",
    "# # # Final Xpath for info?\n",
    "# # '//*[@id=\"house-info\"]/div/div/div[1]/div'\n",
    "\n",
    "# info = pd.Series(info, name='info')\n",
    "\n",
    "# info.isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# address = {}\n",
    "# for j, id_no in enumerate(lines):\n",
    "# #     if j%500==0:\n",
    "# #         print(id_no[0], id_no[1])\n",
    "#     with open('../data/html/'+id_no[0]+'.html', 'r') as f:\n",
    "#         webpage = f.read()       \n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"overview-scroll\"]/div/div/div[2]/div[1]/h1/span/span[1]/span[1]').extract()\n",
    "#     try:\n",
    "#         address[id_no[0]] = s[0].split('<span itemprop=\"streetAddress\" data-rf-test-id=\"abp-streetLine\" class=\"street-address\" title=\"')[1].split('</span>',1)[0]\n",
    "#     except:\n",
    "#         print(id_no[0], id_no[1])\n",
    "#         address[id_no[0]] = None\n",
    "#         pass\n",
    "\n",
    "# address = pd.Series(address, name='address')\n",
    "\n",
    "# details['address'] = details['address'].str.split('\">', expand=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city = {}\n",
    "# for j, id_no in enumerate(lines):\n",
    "# #     if j%500==0:\n",
    "# #         print(id_no[0], id_no[1])\n",
    "#     with open('../data/html/'+id_no[0]+'.html', 'r') as f:\n",
    "#         webpage = f.read()       \n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"overview-scroll\"]/div/div/div[2]/div[1]/h1/span/span[1]/span[2]/span[1]').extract()\n",
    "#     try:\n",
    "#         city[id_no[0]] = s[0].split('<span itemprop=\"addressLocality\" class=\"locality\">')[1].split('<!-- -->')[0]\n",
    "#     except:\n",
    "#         print(id_no[0], id_no[1])\n",
    "#         city[id_no[0]] = None\n",
    "#         pass\n",
    "\n",
    "# city = pd.Series(city, name='city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract zipcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = {}\n",
    "# for j, id_no in enumerate(lines):\n",
    "# #     if j%500==0:\n",
    "# #         print(id_no[0], id_no[1])\n",
    "#     with open('../data/html/'+id_no[0]+'.html', 'r') as f:\n",
    "#         webpage = f.read()       \n",
    "#     s = Selector(text=webpage).xpath('//*[@id=\"overview-scroll\"]/div/div/div[2]/div[1]/h1/span/span[1]/span[2]/span[2]').extract()\n",
    "#     try:\n",
    "#         state[id_no[0]] = s[0].split('class=\"region\">')[1].split('</span>')[0]\n",
    "#     except:\n",
    "#         print(id_no[0], id_no[1])\n",
    "#         state[id_no[0]] = None\n",
    "#         pass\n",
    "\n",
    "# state = pd.Series(state, name='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5110 https://www.redfin.com/ID/Meridian/4121-W-Ladle-Rapids-St-83646/home/144771047\n",
      "11104 https://www.redfin.com/TX/Unknown/13809-Quiet-Fox-Ln-78245/home/148692851\n",
      "18593 https://www.redfin.com/FL/St-Petersburg/5340-62nd-Ave-S-33715/home/47974178\n",
      "18708 https://www.redfin.com/Unknown/Acton/3534-Sierra-Hwy-Unknown/home/145305188\n"
     ]
    }
   ],
   "source": [
    "zipcode = {}\n",
    "for j, id_no in enumerate(lines):\n",
    "#     if j%500==0:\n",
    "#         print(id_no[0], id_no[1])\n",
    "    with open('../data/html/'+id_no[0]+'.html', 'r') as f:\n",
    "        webpage = f.read()       \n",
    "    s = Selector(text=webpage).xpath('//*[@id=\"overview-scroll\"]/div/div/div[2]/div[1]/h1/span/span[1]/span[2]/span[3]').extract()\n",
    "    try:\n",
    "        zipcode[id_no[0]] = s[0].split('class=\"postal-code\">')[1].split('</span>')[0]\n",
    "    except:\n",
    "        print(id_no[0], id_no[1])\n",
    "        zipcode[id_no[0]] = None\n",
    "        pass\n",
    "\n",
    "zipcode = pd.Series(zipcode, name='zipcode')\n",
    "zipcode.to_csv('../data/zipcode_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue working on Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thinkful-py3",
   "language": "python",
   "name": "thinkful-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
